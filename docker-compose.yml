services:
  # Ollama LLM Service
  ollama:
    image: ollama/ollama:latest
    container_name: llm-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    networks:
      - llm-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # LangGraph React Agent Backend
  react-agent:
    build:
      context: ./react-agent
      dockerfile: Dockerfile
    container_name: llm-react-agent
    ports:
      - "2024:2024"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:-ollama}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-http://ollama:11434/v1}
      - OPENWEATHER_API_KEY=${OPENWEATHER_API_KEY:-}
    volumes:
      - ./react-agent:/app
      - /app/.venv
    restart: unless-stopped
    networks:
      - llm-network
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:2024/ok"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Next.js Chat UI Frontend
  agent-chat-ui:
    build:
      context: ./agent-chat-ui
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_API_URL=http://10.40.217.195:2024
        - NEXT_PUBLIC_ASSISTANT_ID=fe096781-5601-53d2-b2f6-0d3403f7e9ca
    container_name: llm-agent-chat-ui
    ports:
      - "3000:3000"
    volumes:
      - ./agent-chat-ui/public:/app/public
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://10.40.217.195:2024
      - NEXT_PUBLIC_ASSISTANT_ID=fe096781-5601-53d2-b2f6-0d3403f7e9ca
      - LANGGRAPH_API_URL=http://react-agent:2024
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY:-}
    depends_on:
      react-agent:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge

volumes:
  ollama-data:
